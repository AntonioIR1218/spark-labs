from pymongo import MongoClient
import streamlit as st
import pandas as pd
import requests
import json

# Configuraci√≥n personalizada para tu proyecto
CONFIG = {
    "JSON_URL": "https://raw.githubusercontent.com/AntonioIR1218/spark-labs/refs/heads/main/results/acoustic_tracks.json",
    "PRODUCER_URL": "https://producer-postgres.onrender.com/send-tracks",
    "PRODUCER_ACOUSTIC_URL": "https://producer-mongodb.onrender.com/send-tracks",
    "GITHUB_REPO_DEFAULT": "spark-labs",
    "GITHUB_USER_DEFAULT": "AntonioIR1218",
    "COLLECTION_NAME": "ExplicitEnergyTracks",
    "DB_NAME": "musica"
}

def send_request(url, headers=None, payload=None):
    """Funci√≥n gen√©rica para enviar requests HTTP"""
    try:
        response = requests.post(url, json=payload, headers=headers or {})
        response.raise_for_status()  
        return response.json(), None
    except requests.RequestException as e:
        return None, str(e)

def post_spark_job(user, repo, job, token, codeurl, dataseturl):
    """Env√≠a un trabajo Spark a GitHub Actions - Versi√≥n simplificada con debug"""
    url = f'https://api.github.com/repos/{user}/{repo}/dispatches'
    headers = {
        'Authorization': f'Bearer {token}',
        'Accept': 'application/vnd.github.v3+json',
        'Content-type': 'application/json'
    }
    payload = {
        "event_type": job,
        "client_payload": {
            "codeurl": codeurl,
            "dataseturl": dataseturl
        }
    }
    
    # Mostrar informaci√≥n de depuraci√≥n
    st.write("### Detalles de la solicitud")
    st.write("**URL:**", url)
    st.write("**Headers:**", headers)
    st.write("**Payload:**", payload)
    
    # Hacer la solicitud POST
    response = requests.post(url, json=payload, headers=headers)
    
    # Mostrar la respuesta
    st.write("**Respuesta del servidor:**")
    st.write("Status Code:", response.status_code)
    st.write("Response Text:", response.text)
    
    if response.status_code == 204:
        st.success("‚úÖ Trabajo de Spark enviado correctamente!")
        st.balloons()
    else:
        st.error(f"‚ùå Error al enviar trabajo Spark: {response.text}")

def get_spark_results(url_results):
    """Obtiene resultados del job de Spark"""
    response = requests.get(url_results)
    st.write("**Respuesta del servidor:**")
    st.write("Status Code:", response.status_code)
    
    if response.status_code == 200:
        try:
            results = response.json()
            st.success("‚úÖ Resultados obtenidos correctamente!")
            
            # Convertir a DataFrame si es posible
            if isinstance(results, list):
                df = pd.DataFrame(results)
                st.dataframe(df)
            
            st.json(results)
        except ValueError:
            st.write("Contenido:", response.text)
    else:
        st.error(f"‚ùå Error al obtener resultados: {response.text}")

# [Mant√©n todas las dem√°s funciones exactamente igual]
def process_tracks_to_kafka_postgres():
    """Env√≠a datos al producer de Kafka para PostgreSQL"""
    with st.status("Procesando datos..."):
        st.write("üåê Conectando con Producer Postgres...")
        result, error = send_request(CONFIG["PRODUCER_URL"])
        
        if error:
            st.error(f"‚ùå Error: {error}")
        else:
            st.success("‚úÖ Datos enviados correctamente a Kafka (Postgres)!")
            st.info(f"üìä Respuesta: {result.get('message', '')}")

def get_data_from_postgres():
    """Obtiene datos desde PostgreSQL"""
    try:
        with st.status("Conectando a PostgreSQL..."):
            conn = st.connection("postgres", type="sql")
            query = "SELECT * FROM acoustic_tracks;"
            df = conn.query(query, ttl=600)

            if df.empty:
                st.warning("‚ö†Ô∏è No se encontraron datos en PostgreSQL")
                return

            st.success(f"‚úÖ Obtenidos {len(df)} registros desde PostgreSQL üêò")
            st.dataframe(df)

            st.subheader("üìä M√©tricas de pistas ac√∫sticas")
            cols = st.columns(3)
            cols[0].metric("Total pistas", len(df))
            cols[1].metric("Duraci√≥n promedio", f"{round(df['duration_ms'].mean()/1000,1)} seg")
            cols[2].metric("Expl√≠citas", f"{df['explicit'].sum()} ({df['explicit'].mean()*100:.1f}%)")

    except Exception as e:
        st.error(f"‚ö†Ô∏è Error al conectar con PostgreSQL: {str(e)}")

def process_tracks_to_kafka_mongo():
    """Env√≠a datos al producer de Kafka para MongoDB"""
    with st.status("Procesando datos..."):
        st.write("üåê Conectando con Producer MongoDB...")
        result, error = send_request(CONFIG["PRODUCER_ACOUSTIC_URL"])
        
        if error:
            st.error(f"‚ùå Error: {error}")
        else:
            st.success("‚úÖ Datos enviados correctamente a Kafka (MongoDB)!")
            st.info(f"üìä Respuesta: {result.get('message', '')}")

def get_data_from_mongo():
    try:
        with st.status("üîç Conectando a MongoDB...", expanded=True) as status:
            # 1. Conexi√≥n
            client = MongoClient(st.secrets["mongodb"]["uri"])
            st.success("‚úÖ Conexi√≥n exitosa a MongoDB Atlas")
            
            # 2. Verificar DB
            db = client[CONFIG["DB_NAME"]]
            
            # 3. Verificar colecci√≥n
            collection = db[CONFIG["COLLECTION_NAME"]]
            
            # 4. Obtener TODOS los datos (sin l√≠mite)
            all_data = list(collection.find({}))
            
            if not all_data:
                status.update(label="‚ö†Ô∏è La colecci√≥n existe pero est√° vac√≠a", state="complete")
                return
                
            # 5. Convertir a DataFrame
            df = pd.DataFrame(all_data)
            
            # Eliminar columna _id si existe
            if '_id' in df.columns:
                df = df.drop(columns=['_id'])
            
            status.update(label=f"‚úÖ Obtenidos {len(df)} documentos", state="complete")
            
            # 6. Mostrar en una secci√≥n ampliada
            st.subheader("üìä Datos completos", divider="rainbow")
            
            # Configurar el dataframe para que ocupe m√°s espacio
            st.dataframe(
                df,
                height=600,  # Altura aumentada
                width=1200,  # Ancho aumentado
                use_container_width=True,
                hide_index=True
            )
            
            # 7. Mostrar estad√≠sticas detalladas
            st.subheader("üìà Estad√≠sticas completas")
            
            cols = st.columns(4)
            cols[0].metric("Total documentos", len(df))
            
            if 'duration_ms' in df.columns:
                duration_min = df['duration_ms'].min()/1000
                duration_max = df['duration_ms'].max()/1000
                duration_avg = df['duration_ms'].mean()/1000
                cols[1].metric("Duraci√≥n (seg)", 
                              f"{duration_avg:.1f} (avg)",
                              delta=f"{duration_min:.1f}-{duration_max:.1f}")
            
            if 'explicit' in df.columns:
                explicit_count = df['explicit'].sum()
                explicit_percent = (explicit_count/len(df))*100
                cols[2].metric("Contenido expl√≠cito", 
                              f"{explicit_count} ({explicit_percent:.1f}%)")
            
            if 'artists' in df.columns:
                unique_artists = df['artists'].explode().nunique()
                cols[3].metric("Artistas √∫nicos", unique_artists)
            
            # 8. Mostrar estructura de datos
            with st.expander("üîç Estructura completa de los datos", expanded=True):
                st.json(all_data[0] if all_data else {})
                
            # 9. Opci√≥n para descargar los datos
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="‚¨áÔ∏è Descargar datos como CSV",
                data=csv,
                file_name='acoustic_tracks.csv',
                mime='text/csv'
            )

    except Exception as e:
        st.error(f"‚ùå Error cr√≠tico: {str(e)}")
        st.info("""
        Verifica:
        1. Tu conexi√≥n a internet
        2. El archivo secrets.toml
        3. Los permisos en Atlas
        4. Que la colecci√≥n tenga datos
        """)

# Interfaz de la aplicaci√≥n
st.set_page_config(page_title="Dashboard Musical", layout="wide")
st.title("üéµ Acoustic Tracks Analytics Dashboard")

tab1, tab2, tab3 = st.tabs(["Spark Jobs", "Kafka/PostgreSQL", "Kafka/MongoDB"])

with tab1:
    st.header("‚ö° Gesti√≥n de Trabajos Spark")
    
    # Formulario simplificado para Spark
    col1, col2 = st.columns(2)
    with col1:
        github_user = st.text_input('GitHub user', value=CONFIG["GITHUB_USER_DEFAULT"])
        github_repo = st.text_input('GitHub repo', value=CONFIG["GITHUB_REPO_DEFAULT"])
        spark_job = st.text_input('Spark job name', value='spark')
    
    with col2:
        github_token = st.text_input('GitHub token', type='password')
        code_url = st.text_input('Code URL', 
                               value='https://raw.githubusercontent.com/AntonioIR1218/spark-labs/refs/heads/main/musica.py')
        dataset_url = st.text_input('Dataset URL', 
                                  value='https://raw.githubusercontent.com/AntonioIR1218/spark-labs/refs/heads/main/dataset.csv')
    
    if st.button("üöÄ Ejecutar Spark Job", key="spark_submit"):
        post_spark_job(github_user, github_repo, spark_job, github_token, code_url, dataset_url)
    
    st.divider()
    st.header("üìä Resultados del Spark Job")
    url_results = st.text_input('URL de resultados', 
                              value=CONFIG["JSON_URL"])
    
    if st.button("üîÑ Obtener Resultados", key="get_results"):
        get_spark_results(url_results)

with tab2:
    st.header("üìä Pipeline: Kafka ‚Üí PostgreSQL")
    st.caption("Env√≠a y visualiza datos de pistas ac√∫sticas en PostgreSQL")
    
    cols = st.columns([1, 1, 2])
    with cols[0]:
        if st.button("üîÑ Cargar datos a Kafka", key="kafka_postgres", use_container_width=True):
            process_tracks_to_kafka_postgres()
    
    with cols[1]:
        if st.button("üì• Obtener datos de PostgreSQL", use_container_width=True):
            get_data_from_postgres()
    
   

with tab3:
    st.header("üìä Pipeline: Kafka ‚Üí MongoDB")
    st.caption("Env√≠a y visualiza datos de pistas ac√∫sticas en MongoDB")
    
    cols = st.columns([1, 1, 2])
    with cols[0]:
        if st.button("üîÑ Cargar datos a Kafka", key="kafka_mongo", use_container_width=True):
            process_tracks_to_kafka_mongo()
    
    with cols[1]:
        if st.button("üì• Obtener datos de MongoDB", use_container_width=True):
            get_data_from_mongo()

# Notas adicionales
st.sidebar.markdown("## üîç Acerca de")
st.sidebar.info("""
Este dashboard permite:
- Procesar datos musicales con Spark
- Distribuir datos a trav√©s de Kafka
- Almacenar en PostgreSQL y MongoDB
- Visualizar m√©tricas clave
""")

st.sidebar.markdown("## ‚öôÔ∏è Configuraci√≥n")
st.sidebar.write(f"**Repositorio:** {CONFIG['GITHUB_REPO_DEFAULT']}")
st.sidebar.write(f"**Dataset:** [acoustic_tracks.json]({CONFIG['JSON_URL']})")
